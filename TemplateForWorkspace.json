{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "rksynapseworkspace03"
		},
		"AzureBlobStorage1_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'AzureBlobStorage1'"
		},
		"OnpremisesSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'OnpremisesSqlServer'"
		},
		"rksynapseanal-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'rksynapseanal-WorkspaceDefaultSqlServer'"
		},
		"rksynapseworkspace03-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'rksynapseworkspace03-WorkspaceDefaultSqlServer'"
		},
		"rksynapseanal-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://rkadlsanal.dfs.core.windows.net"
		},
		"rksynapseworkspace03-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://rkadlssynapse.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/PipelineFileCopy')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "CopyTextfile test",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "DelimitedTextSource",
								"storeSettings": {
									"type": "AzureBlobStorageReadSettings",
									"recursive": true,
									"wildcardFileName": "*.txt",
									"enablePartitionDiscovery": false
								},
								"formatSettings": {
									"type": "DelimitedTextReadSettings"
								}
							},
							"sink": {
								"type": "DelimitedTextSink",
								"storeSettings": {
									"type": "AzureBlobStorageWriteSettings"
								},
								"formatSettings": {
									"type": "DelimitedTextWriteSettings",
									"quoteAllText": true,
									"fileExtension": ".txt"
								}
							},
							"enableStaging": false,
							"translator": {
								"type": "TabularTranslator",
								"typeConversion": true,
								"typeConversionSettings": {
									"allowDataTruncation": true,
									"treatBooleanAsNumber": false
								}
							}
						},
						"inputs": [
							{
								"referenceName": "DelimitedText2",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "DelimitedText3",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					},
					{
						"name": "Delete1",
						"type": "Delete",
						"dependsOn": [
							{
								"activity": "CopyTextfile test",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "DelimitedText2",
								"type": "DatasetReference",
								"parameters": {}
							},
							"logStorageSettings": {
								"linkedServiceName": {
									"referenceName": "AzureBlobStorage1",
									"type": "LinkedServiceReference"
								},
								"path": "output"
							},
							"enableLogging": true,
							"storeSettings": {
								"type": "AzureBlobStorageReadSettings",
								"recursive": true,
								"wildcardFileName": "*.txt",
								"enablePartitionDiscovery": false
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": [],
				"lastPublishTime": "2021-09-27T15:27:19Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/DelimitedText2')]",
				"[concat(variables('workspaceId'), '/datasets/DelimitedText3')]",
				"[concat(variables('workspaceId'), '/linkedServices/AzureBlobStorage1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DelimitedText1')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureBlobStorage1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobStorageLocation",
						"fileName": {
							"value": "@concat(pipeline().Runid,'.txt')",
							"type": "Expression"
						},
						"folderPath": "fromonprem",
						"container": "rkadlssynapse"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureBlobStorage1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DelimitedText2')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureBlobStorage1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobStorageLocation",
						"container": "input"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureBlobStorage1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DelimitedText3')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureBlobStorage1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobStorageLocation",
						"container": "output"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureBlobStorage1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SqlServerTable1')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "OnpremisesSqlServer",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "SqlServerTable",
				"schema": [
					{
						"name": "ID",
						"type": "int",
						"precision": 10
					},
					{
						"name": "FirstName",
						"type": "varchar"
					},
					{
						"name": "LastName",
						"type": "varchar"
					}
				],
				"typeProperties": {
					"schema": "dbo",
					"table": "Emp"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/OnpremisesSqlServer')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ds_orders_csv_source')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureBlobStorage1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobStorageLocation",
						"fileName": "2016Orders.csv",
						"container": "ordersdata"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "18",
						"type": "String"
					},
					{
						"name": "8",
						"type": "String"
					},
					{
						"name": "4/16/2016",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureBlobStorage1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureBlobStorage1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"connectionString": "[parameters('AzureBlobStorage1_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/OnpremisesSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "SqlServer",
				"typeProperties": {
					"connectionString": "[parameters('OnpremisesSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "SHIR",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/SHIR')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/rksynapseanal-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('rksynapseanal-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/rksynapseanal-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('rksynapseanal-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/rksynapseworkspace03-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('rksynapseworkspace03-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/rksynapseworkspace03-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('rksynapseworkspace03-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				},
				"managedVirtualNetwork": {
					"type": "ManagedVirtualNetworkReference",
					"referenceName": "default"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/managedVirtualNetworks/default')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SHIR')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "SelfHosted",
				"description": "self hosted",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 1')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://rkadlssynapse.dfs.core.windows.net/rkadlssynapse/NYCTripSmall.parquet',\n        FORMAT='PARQUET'\n    ) AS [result]\n\nSELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n            BULK '/rkadlssynapse/NYCTripSmall.parquet',\n            DATA_SOURCE = 'rkadlslake',\n            FORMAT='PARQUET'\n    ) AS [result]\n\n\nCREATE DATABASE DataExplorationDB COLLATE Latin1_General_100_BIN2_UTF8\n\n\ncreate external data source rkadlslake \nwith ( location  = 'https://rkadlssynapse.blob.core.windows.net')\n\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "DataExplorationDB",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 2')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "CREATE DATABASE SCOPED CREDENTIAL sqlondemand1\n\nWITH IDENTITY='SHARED ACCESS SIGNATURE',  \n\nSECRET = 'sv=2018-03-28&ss=bf&srt=sco&sp=rl&st=2019-10-14T12%3A10%3A25Z&se=2061-12-31T12%3A10%3A00Z&sig=KlSU2ullCscyTS0An0nozEpo4tO5JAgGBvw%2FJX2lguw%3D'\n\nGO\n\nCREATE EXTERNAL DATA SOURCE SqlOnDemandDemo WITH (\n\n    LOCATION = 'https://sqlondemandstorage.blob.core.windows.net',\n\n    CREDENTIAL = sqlondemand1\n\n);\n\n\nSELECT TOP 10 *\nFROM OPENROWSET\n  (\n      BULK 'csv/population/*.csv',\n      DATA_SOURCE = 'SqlOnDemandDemo',\n      FORMAT = 'CSV', PARSER_VERSION = '2.0'\n  )\nWITH\n  (\n      country_code VARCHAR (5)\n    , country_name VARCHAR (100)\n    , year smallint\n    , population bigint\n  ) AS r\nWHERE\n  country_name = 'Luxembourg' AND year = 2017",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "DataExplorationDB",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 3')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT * FROM OrdersPartition",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "rksqldpool03",
						"poolName": "rksqldpool03"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 4')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT   o.name AS Table_name,  pnp.partition_number AS Partition_number,  sum(pnp.rows) AS Row_count  FROM sys.pdw_nodes_partitions AS pnp     JOIN sys.pdw_nodes_tables AS NTables ON pnp.object_id = NTables.object_id       AND pnp.pdw_node_id = NTables.pdw_node_id     JOIN sys.pdw_table_mappings AS TMap ON NTables.name = TMap.physical_name       AND substring(TMap.physical_name,40, 10) = pnp.distribution_id    JOIN sys.objects AS o ON TMap.object_id = o.object_id    WHERE o.name in ('OrdersPartition')   GROUP BY partition_number, o.name, pnp.data_compression_desc;",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "rksqldpool03",
						"poolName": "rksqldpool03"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 5')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "drop table [dbo].[Orders]\nCREATE TABLE [dbo].[Orders] ([OrderID] [int] NOT NULL,\n[CustomerID] [int] NOT NULL,  [OrderDate] [date] NOT NULL ) WITH\n(  DISTRIBUTION = ROUND_ROBIN,  CLUSTERED COLUMNSTORE INDEX )\nGO\n\nselect * from [dbo].[Orders] ",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "rksqldpool03",
						"poolName": "rksqldpool03"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 6')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT TOP (100) [OrderID]\n,[CustomerID]\n,[OrderDate]\n FROM [dbo].[Orders]",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "rksqldpool03",
						"poolName": "rksqldpool03"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Getting Started with dotNET for Apache Spark')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "rksparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_sparkdotnet",
						"display_name": "Synapse SparkDotNet"
					},
					"language_info": {
						"name": "csharp"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/aa62e2f1-b843-4840-bcdf-64081ba7e680/resourceGroups/Day4RG/providers/Microsoft.Synapse/workspaces/rksynapseworkspace03/bigDataPools/rksparkPool",
						"name": "rksparkPool",
						"type": "Spark",
						"endpoint": "https://rksynapseworkspace03.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/rksparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"# Hitchhiker's Guide to .NET for Apache Spark\n",
							"\n",
							"Welcome to the .NET for Apache Spark tutorial! We are glad to have you here. Before we begin, let us cover answers to a few quick questions:\n",
							"\n",
							" - #### What is .NET for Apache Spark?\n",
							"  .NET for Apache Spark provides high performance APIs for using Apache Spark from C# and F#. With these .NET APIs, you can access the most popular Dataframe and SparkSQL aspects of Apache Spark, for working with structured data, and Spark Structured Streaming, for working with streaming data.\n",
							"\n",
							"  .NET for Apache Spark is compliant with .NET Standard - a formal specification of .NET APIs that are common across .NET implementations. This means you can use .NET for Apache Spark anywhere you write .NET code allowing you to reuse all the knowledge, skills, code, and libraries you already have as a .NET developer.\n",
							"\n",
							" - #### Where can I find more on .NET for Apache Spark?\n",
							"  https://github.com/dotnet/spark\n",
							"\n",
							" - #### I did not know there was a REPL for C#!?\n",
							"   Great question! :) We collaborated with the .NET team and they built one for us! https://github.com/dotnet/interactive \n",
							"\n",
							"Whew! Now that we have covered some basic information, let's begin! \n",
							"\n",
							"Since the .NET REPL is something very new, let us start by exploring what you can do with the REPL. "
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"# Basic Capabilities of the C# REPL"
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"// Simple assignments should just work \n",
							"var x = 1 + 25;"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"// You can either use traditional approach to printing a variable...\n",
							"Console.WriteLine(x);\n",
							"\n",
							"// ... or just type it and execute a cell\n",
							"256"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"// You can even play with built-in libraries/functions\n",
							"Enumerable.Range(1, 5)"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"// And now for some C# 8.0 features. If you haven't read it,\n",
							"// here's the link: \n",
							"// https://docs.microsoft.com/en-us/dotnet/csharp/whats-new/csharp-8\n",
							"1..4"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"// We can even do pattern matching!\n",
							"public static string RockPaperScissors(string first, string second)\n",
							"    => (first, second) switch\n",
							"    {\n",
							"        (\"rock\", \"paper\") => \"rock is covered by paper. Paper wins.\", // <-- Next cell prints this out\n",
							"        (\"rock\", \"scissors\") => \"rock breaks scissors. Rock wins.\",\n",
							"        (\"paper\", \"rock\") => \"paper covers rock. Paper wins.\",\n",
							"        (\"paper\", \"scissors\") => \"paper is cut by scissors. Scissors wins.\",\n",
							"        (\"scissors\", \"rock\") => \"scissors is broken by rock. Rock wins.\",\n",
							"        (\"scissors\", \"paper\") => \"scissors cuts paper. Scissors wins.\",\n",
							"        (_, _) => \"tie\"\n",
							"    };"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"RockPaperScissors(\"rock\", \"paper\")"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"// Now, for the fun part! You can render HTML\n",
							"display(\n",
							"    div(\n",
							"        h1(\"Our Incredibly Declarative Example\"),\n",
							"        p(\"Can you believe we wrote this \", b(\"in C#\"), \"?\"),\n",
							"        img[src:\"https://media.giphy.com/media/xUPGcguWZHRC2HyBRS/giphy.gif\"],\n",
							"        p(\"What will \", b(\"you\"), \" create next?\")\n",
							"    )\n",
							");"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"# Looking at data through Spark.NET\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"// Let us use some sample data. In this cell, we create this data \n",
							"// from *scratch* but you can also load it from your storage container. \n",
							"// For instance, \n",
							"// var df = spark.Read().Json(\"wasbs://<account>@<container>.blob.core.windows.net/people.json\");\n",
							"\n",
							"using Microsoft.Spark.Sql;\n",
							"using Microsoft.Spark.Sql.Types;\n",
							"using static Microsoft.Spark.Sql.Functions;\n",
							"\n",
							"var schema = new StructType(new List<StructField>()\n",
							"    {\n",
							"        new StructField(\"id\", new IntegerType()),\n",
							"        new StructField(\"name\", new StringType())\n",
							"    });\n",
							"\n",
							"var data = new List<GenericRow>();\n",
							"data.Add(new GenericRow(new object[] { 0,  \"Michael\" }));\n",
							"data.Add(new GenericRow(new object[] { 1,  \"Elva\"    }));\n",
							"data.Add(new GenericRow(new object[] { 2,  \"Terry\"   }));\n",
							"data.Add(new GenericRow(new object[] { 3,  \"Steve\"   }));\n",
							"data.Add(new GenericRow(new object[] { 4,  \"Brigit\"  }));\n",
							"data.Add(new GenericRow(new object[] { 5,  \"Niharika\"}));\n",
							"data.Add(new GenericRow(new object[] { 6,  \"Rahul\"   }));\n",
							"data.Add(new GenericRow(new object[] { 7,  \"Tomas\"   }));\n",
							"data.Add(new GenericRow(new object[] { 8,  \"Euan\"   }));\n",
							"data.Add(new GenericRow(new object[] { 9,  \"Lev\"   }));\n",
							"data.Add(new GenericRow(new object[] { 10, \"Saveen\"   }));\n",
							"\n",
							"var df = spark.CreateDataFrame(data, schema);\n",
							"df.Show();"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"// Wait, that rendering is old-school plain! Let's spice things up a bit!\n",
							"// What we're doing here is to define a specific formatter that is tied to \n",
							"// Microsoft.Spark.Sql.DataFrame and registering it. When we then invoke\n",
							"// display() and pass a DataFrame, the formatter is invoked, which then\n",
							"// generates the necessary HTML\n",
							"\n",
							"Microsoft.DotNet.Interactive.Formatting.Formatter<Microsoft.Spark.Sql.DataFrame>.Register((df, writer) =>\n",
							"{\n",
							"    var headers = new List<dynamic>();\n",
							"    var columnNames = df.Columns();\n",
							"    headers.Add(th(i(\"index\")));\n",
							"    headers.AddRange(columnNames.Select(c => th(c)));\n",
							"\n",
							"    var rows = new List<List<dynamic>>();\n",
							"    var currentRow = 0;\n",
							"    var dfRows = df.Take(Math.Min(20, (int)df.Count()));\n",
							"    foreach (Row dfRow in dfRows)\n",
							"    {\n",
							"        var cells = new List<dynamic>();\n",
							"        cells.Add(td(currentRow));\n",
							"\n",
							"        foreach (string columnName in columnNames)\n",
							"        {\n",
							"            cells.Add(td(dfRow.Get(columnName)));\n",
							"        }\n",
							"\n",
							"        rows.Add(cells);\n",
							"        ++currentRow;\n",
							"    }\n",
							"\n",
							"    var t = table[@border: \"0.1\"](\n",
							"        thead[@style: \"background-color: blue; color: white\"](headers),\n",
							"        tbody[@style: \"color: red\"](rows.Select(r => tr(r))));\n",
							"\n",
							"    writer.Write(t);\n",
							"}, \"text/html\");"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"// Now, let's try rendering the Spark's DataFrame in two ways...\n",
							"\n",
							"// ... a regular way ...\n",
							"df.Show();\n",
							"\n",
							"// Using dotnet-interactive's display method (so it invokes the formatter we just defined)\n",
							"display(df);\n",
							""
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"// ... and just typing df (equivalent to \"display(df);\")\r\n",
							"df"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"// Let us now try something more advanced like, defining C# classes on-the-fly...\n",
							"public static class A {\n",
							"    public static readonly string s = \"The person named \";\n",
							"}"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"// ... and just for illustration, let's define one more simple class\n",
							"public static class B {\n",
							"    private static Random _r = new Random();\n",
							"    private static List<string> _moods = new List<string>{ \"happy\",\"funny\",\"awesome\",\"cool\"};\n",
							"\n",
							"    public static string GetMood() {\n",
							"        return _moods[_r.Next(_moods.Count)];\n",
							"    }\n",
							"}"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"// Let us now define a Spark User-defined Function (UDF) that utilizes\n",
							"// the classes we just defined above. If you do not recognize the syntax\n",
							"// below, here's some relevant documentation:\n",
							"// https://docs.microsoft.com/en-us/dotnet/api/system.func-2?view=netcore-3.1\n",
							"// https://github.com/dotnet/spark/blob/master/examples/Microsoft.Spark.CSharp.Examples/Sql/Batch/Basic.cs\n",
							"//\n",
							"// Note: If you change the class definition above, and execute the cell,\n",
							"// you should re-execute this cell (i.e., the cell that defines the UDF)\n",
							"var udf = Udf<string, string>(str => $\"{A.s} - {str} - is {B.GetMood()}!\");"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"// Let's use the UDF on our Spark DataFrame\n",
							"display(\n",
							"    df\n",
							"    .Select(\n",
							"        udf((Microsoft.Spark.Sql.Column)df[\"name\"])));"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"// Tables are not that interesting, right? :) Let's do some visualizations now.\r\n",
							"// Let us start with something simple to illustrate the idea. We highly encourage\r\n",
							"// you to look at https://fslab.org/XPlot/ to understand how you can use XPlot's\r\n",
							"// full capabilities. While the examples are in F#, it is fairly straightforward\r\n",
							"// to rewrite in C#.\r\n",
							"\r\n",
							"using XPlot.Plotly;\r\n",
							"\r\n",
							"var lineChart = Chart.Line(new List<int> { 1, 2, 3, 4, 5, 6, 10, 44 });\r\n",
							"lineChart.WithTitle(\"My awesome chart\");\r\n",
							"lineChart.WithXTitle(\"X axis\");\r\n",
							"lineChart.WithYTitle(\"Y axis\");\r\n",
							"lineChart"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"// Good! Now let us try to visualize the Spark DataFrame we have.\n",
							"// Now is a good time to refresh your concept of a Spark DataFrame\n",
							"// https://spark.apache.org/docs/latest/sql-programming-guide.html\n",
							"// Remember that a Spark DataFrame is a distributed representation \n",
							"// of your dataset (yes, even if your data is a few KB). Since we\n",
							"// are using a visualization library, we need to first 'collect'\n",
							"// (notice how we are using df.Collect().ToArray() below)\n",
							"// all the data that is distributed on your cluster, and shape it\n",
							"// appropriately for XPlot.\n",
							"//\n",
							"// Note: Visualizations are good for smaller datasets (typically, \n",
							"// a few 10s of thousands of data points coming to KBs), so if you are\n",
							"// trying to visualize GBs of data, it is usually a good idea to\n",
							"// summarize your data appropriately using Spark.NET's APIs. For\n",
							"// a list of summarization APIs, see here:\n",
							"// https://docs.microsoft.com/en-us/dotnet/api/microsoft.spark.sql.functions?view=spark-dotnet\n",
							"\n",
							"var names = new List<string>();\n",
							"var ids = new List<int>();\n",
							"\n",
							"foreach (Row row in df.Collect().ToArray())\n",
							"{\n",
							" names.Add(row.GetAs<string>(\"name\"));\n",
							" int? id = row.GetAs<int?>(\"id\");\n",
							" ids.Add( id ?? 0);\n",
							"}\n",
							"var bar = new Graph.Bar\n",
							"{\n",
							" name = \"bar chart\",\n",
							" x = names,\n",
							" y = ids\n",
							"};\n",
							"\n",
							"var chart = Chart.Plot(new[] {bar});\n",
							"display(chart);"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"// As a final step, let us now plot a histogram of a random dataset\n",
							"\n",
							"using XPlot.Plotly;\n",
							"\n",
							"var schema = new StructType(new List<StructField>()\n",
							"    {\n",
							"        new StructField(\"number\", new DoubleType())\n",
							"    });\n",
							"\n",
							"Random random = new Random();\n",
							"\n",
							"var data = new List<GenericRow>();\n",
							"for(int i = 0; i <=100; i++) {\n",
							"    data.Add(new GenericRow(new object[] { random.NextDouble() }));\n",
							"}\n",
							"\n",
							"var histogramDf = spark.CreateDataFrame(data, schema);\n",
							"histogramDf.Show()"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"// Time to use LINQ (or Language Integrated Query) :)\n",
							"// For those that are not familiar with LINQ, you can read more about it\n",
							"// here: https://docs.microsoft.com/en-us/dotnet/csharp/programming-guide/concepts/linq/\n",
							"\n",
							"using System.Linq;\n",
							"\n",
							"// Let us take the histogramDf we loaded through Spark and sample some data points\n",
							"// for the histogram. We will then use LINQ to shape the data for our next \n",
							"// steps (visualization!)\n",
							"var sample1 = \n",
							"        histogramDf.Sample(0.5, true).Collect().ToArray() // <---- Spark APIs\n",
							"        .Select(x => x.GetAs<double>(\"number\")); // <---- LINQ APIs\n",
							"        \n",
							"// Let us create two more sample sets we can use for plotting\n",
							"var sample2 = histogramDf.Sample(0.3, false).Collect().ToArray().Select(x => x.GetAs<double>(\"number\"));\n",
							"var sample3 = histogramDf.Sample(0.6, true).Collect().ToArray().Select(x => x.GetAs<double>(\"number\"));"
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"// Let us plot the histograms now!\n",
							"var hist1 = new Graph.Histogram{x = sample1, opacity = 0.75};\n",
							"var hist2 = new Graph.Histogram{x = sample2, opacity = 0.75};\n",
							"var hist3 = new Graph.Histogram{x = sample3, opacity = 0.75};"
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"Chart.Plot(new[] {hist1})"
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"Chart.Plot(new[] {hist2})"
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"Chart.Plot(new[] {hist3})"
						],
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"// but wait, that's three different graphs and it's impossible to read them\n",
							"// altogether! Let's try an overlay histogram, shall we?\n",
							"var layout = new XPlot.Plotly.Layout.Layout{barmode=\"overlay\", title=\"Overlaid Histogram\"};\n",
							"var histogram = Chart.Plot(new[] {hist1, hist2, hist3});\n",
							"histogram.WithLayout(layout);\n",
							"histogram"
						],
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"// And for the final touches\n",
							"using static XPlot.Plotly.Graph;\n",
							"\n",
							"layout.title = \"Overlaid Histogram with cool colors!\";\n",
							"hist1.marker = new Marker {color = \"#D65108)\"};\n",
							"hist2.marker = new Marker {color = \"#ffff00\"}; \n",
							"hist3.marker = new Marker {color = \"#462255\"};\n",
							"\n",
							"histogram"
						],
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# VectorUdfs using Apache Arrow\n",
							"Spark .NET supports constructing Arrow-backed VectorUdfs by directly using the [Apache Arrow](https://github.com/apache/arrow) library or by using the [Microsoft DataFrame](https://devblogs.microsoft.com/dotnet/an-introduction-to-dataframe/) library."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"// Let's construct a VectorUdf by directly using Arrow.\r\n",
							"using Apache.Arrow;\r\n",
							"using static Microsoft.Spark.Sql.ArrowFunctions;\r\n",
							"using Column = Microsoft.Spark.Sql.Column;\r\n",
							"\r\n",
							"// Helper method to construct an ArrowArray from a string[].\r\n",
							"public static IArrowArray ToStringArrowArray(string[] array)\r\n",
							"{\r\n",
							"    var valueOffsets = new ArrowBuffer.Builder<int>();\r\n",
							"    var valueBuffer = new ArrowBuffer.Builder<byte>();\r\n",
							"    int offset = 0;\r\n",
							"\r\n",
							"    foreach (string str in array)\r\n",
							"    {\r\n",
							"        byte[] bytes = Encoding.UTF8.GetBytes(str);\r\n",
							"        valueOffsets.Append(offset);\r\n",
							"        valueBuffer.Append(bytes);\r\n",
							"        offset += bytes.Length;\r\n",
							"    }\r\n",
							"\r\n",
							"    valueOffsets.Append(offset);\r\n",
							"    return new StringArray(\r\n",
							"        new ArrayData(\r\n",
							"            Apache.Arrow.Types.StringType.Default,\r\n",
							"            valueOffsets.Length - 1,\r\n",
							"            0,\r\n",
							"            0,\r\n",
							"            new[] { ArrowBuffer.Empty, valueOffsets.Build(), valueBuffer.Build() }));\r\n",
							"}\r\n",
							"\r\n",
							"Func<Int32Array, StringArray, StringArray> arrowUdf =\r\n",
							"    (ids, names) => (StringArray)ToStringArrowArray(\r\n",
							"        Enumerable.Range(0, names.Length)\r\n",
							"            .Select(i => $\"id: {ids.GetValue(i)}, name: {names.GetString(i)}\")\r\n",
							"            .ToArray());\r\n",
							"\r\n",
							"Func<Column, Column, Column> vectorUdf1 = VectorUdf(arrowUdf);"
						],
						"outputs": [],
						"execution_count": 26
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"df.Select(vectorUdf1(df[\"id\"], df[\"name\"]))"
						],
						"outputs": [],
						"execution_count": 27
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"// Now let's construct a VectorUdf by using Microsoft Dataframe\r\n",
							"using Microsoft.Data.Analysis;\r\n",
							"using static Microsoft.Spark.Sql.DataFrameFunctions;\r\n",
							"\r\n",
							"Func<Int32DataFrameColumn, ArrowStringDataFrameColumn, ArrowStringDataFrameColumn> msftDfFunc =\r\n",
							"    (ids, names) =>\r\n",
							"    {\r\n",
							"        long i = 0;\r\n",
							"        return names.Apply(cur => $\"id: {ids[i++]}, name: {cur}\");\r\n",
							"    };\r\n",
							"\r\n",
							"Func<Column, Column, Column> vectorUdf2 = VectorUdf(msftDfFunc);"
						],
						"outputs": [],
						"execution_count": 28
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"df.Select(vectorUdf2(df[\"id\"], df[\"name\"]))"
						],
						"outputs": [],
						"execution_count": 29
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Running custom Nugets as UDFs inside Spark\r\n",
							"In .NET for Spark, it is very easy to install a library from Nuget and use in UDFs in Spark."
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"// Use #r to install new packages into the current session\r\n",
							"\r\n",
							"// Installs latest version\r\n",
							"#r \"nuget: MathNet.Numerics\"\r\n",
							"\r\n",
							"// Installs specified version\r\n",
							"#r \"nuget: NumSharp,0.20.5\""
						],
						"outputs": [],
						"execution_count": 30
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"// Let's construct some Udfs that have a dependency on the installed packages.\r\n",
							"using MathNet.Numerics.LinearAlgebra;\r\n",
							"using MathNet.Numerics.LinearAlgebra.Double;\r\n",
							"using NumSharp;\r\n",
							"\r\n",
							"var mathNetUdf = Udf<string, string>(str => {\r\n",
							"    Matrix<double> matrix = DenseMatrix.OfArray(new double[,] {\r\n",
							"        {1,1,1,1},\r\n",
							"        {1,2,3,4},\r\n",
							"        {4,3,2,1}});\r\n",
							"\r\n",
							"    return $\"{matrix[0, 0]} - {str} - {matrix[1, 1]}!\";\r\n",
							"});\r\n",
							"\r\n",
							"var numSharpUdf = Udf<string, string>(str => {\r\n",
							"    var nd = np.arange(12);\r\n",
							"\r\n",
							"    return $\"{nd[1].ToString()} - {str} - {nd[5].ToString()}!\";\r\n",
							"});"
						],
						"outputs": [],
						"execution_count": 31
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"// UDFs are run on the Microsoft.Spark.Worker process. The package assemblies\r\n",
							"// defined as a Udf depedency are shipped to the Worker so they are available\r\n",
							"// at the time of execution.\r\n",
							"df.Select(mathNetUdf(df[\"name\"])).Show();\r\n",
							"\r\n",
							"df.Select(numSharpUdf(df[\"name\"])).Show();\r\n",
							"\r\n",
							"// We can also chain udfs.\r\n",
							"df.Select(mathNetUdf(numSharpUdf(df[\"name\"])))"
						],
						"outputs": [],
						"execution_count": 32
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Synapse Spark Utility Methods\r\n",
							"[Microsoft.Spark.Extensions.Azure.Synapse.Analytics.Notebook.MSSparkUtils](dev.azure.com/dnceng/internal/_git/dotnet-spark-extensions?path=%2Fsrc%2FMicrosoft.Spark.Extensions.Azure.Synapse.Analytics%2FNotebook%2FMSSparkUtils)"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"// Utility for obtaining credentials (tokens and keys) for Synapse resources.\r\n",
							"// Credentials methods https://dev.azure.com/dnceng/internal/_git/dotnet-spark-extensions?path=%2Fsrc%2FMicrosoft.Spark.Extensions.Azure.Synapse.Analytics%2FNotebook%2FMSSparkUtils%2FCredentials.cs\r\n",
							"using Microsoft.Spark.Extensions.Azure.Synapse.Analytics.Notebook.MSSparkUtils;\r\n",
							"\r\n",
							"// Note that the help message is the help message returned by the Scala implementation. This needs to be updated and addressed in a future version.\r\n",
							"Console.WriteLine($\"Help:\\n{Credentials.Help()}\");"
						],
						"outputs": [],
						"execution_count": 33
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"// Utility for obtaining environment metadata for Synapse.\r\n",
							"// Env methods https://dev.azure.com/dnceng/internal/_git/dotnet-spark-extensions?path=%2Fsrc%2FMicrosoft.Spark.Extensions.Azure.Synapse.Analytics%2FNotebook%2FMSSparkUtils%2FEnv.cs\r\n",
							"Console.WriteLine($\"UserName: {Env.GetUserName()}\");\r\n",
							"Console.WriteLine($\"UserId: {Env.GetUserId()}\");\r\n",
							"Console.WriteLine($\"WorkspaceName: {Env.GetWorkspaceName()}\");\r\n",
							"Console.WriteLine($\"PoolName: {Env.GetPoolName()}\");\r\n",
							"Console.WriteLine($\"ClusterId: {Env.GetClusterId()}\");\r\n",
							"\r\n",
							"// Note that the help message is the help message returned by the Scala implementation. This needs to be updated and addressed in a future version.\r\n",
							"Console.WriteLine($\"Help:\\n{Env.Help()}\");"
						],
						"outputs": [],
						"execution_count": 34
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"// Utility for filesystem operations in Synapse notebook\r\n",
							"// FS methods https://dev.azure.com/dnceng/internal/_git/dotnet-spark-extensions?path=%2Fsrc%2FMicrosoft.Spark.Extensions.Azure.Synapse.Analytics%2FNotebook%2FMSSparkUtils%2FFS.cs\r\n",
							"// FileInfo methods https://dev.azure.com/dnceng/internal/_git/dotnet-spark-extensions?path=%2Fsrc%2FMicrosoft.Spark.Extensions.Azure.Synapse.Analytics%2FNotebook%2FMSSparkUtils%2FFileInfo.cs\r\n",
							"\r\n",
							"// Note that the help message is the help message returned by the Scala implementation. This needs to be updated and addressed in a future version.\r\n",
							"FS.Help(\"\");"
						],
						"outputs": [],
						"execution_count": 35
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"// Utility for notebook operations (e.g, chaining Synapse notebooks together)\r\n",
							"// Notebook methods https://dev.azure.com/dnceng/internal/_git/dotnet-spark-extensions?path=%2Fsrc%2FMicrosoft.Spark.Extensions.Azure.Synapse.Analytics%2FNotebook%2FMSSparkUtils%2FNotebook.cs\r\n",
							"\r\n",
							"// Note that the help message is the help message returned by the Scala implementation. This needs to be updated and addressed in a future version.\r\n",
							"Notebook.Help(\"\");"
						],
						"outputs": [],
						"execution_count": 36
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# [Microsoft.Spark.Extensions.Azure.Synapse.Analytics.Notebook.Visualization](https://dev.azure.com/dnceng/internal/_git/dotnet-spark-extensions?path=%2Fsrc%2FMicrosoft.Spark.Extensions.Azure.Synapse.Analytics%2FNotebook%2FVisualization%2FFunctions.cs)"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"using Microsoft.Spark.Extensions.Azure.Synapse.Analytics.Notebook.Visualization;\r\n",
							"// Construct an specific html fragment to synapse notebook front-end for rendering\r\n",
							"// based on user-input html content.\r\n",
							"DisplayHTML(\"<h1>Hello World</h1>\");"
						],
						"outputs": [],
						"execution_count": 37
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# [TokenLibrary](https://dev.azure.com/dnceng/internal/_git/dotnet-spark-extensions?path=%2Fsrc%2FMicrosoft.Spark.Extensions.Azure.Synapse.Analytics%2FUtils%2FTokenLibrary.cs)\r\n",
							"\r\n",
							"[Synapse Analytics TokenLibrary Official Docs](https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-secure-credentials-with-tokenlibrary)"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"using Microsoft.Spark.Extensions.Azure.Synapse.Analytics.Utils;\r\n",
							"\r\n",
							"// Note that the help message is the help message returned by the Scala implementation. This needs to be updated and addressed in a future version.\r\n",
							"// TODO: Methodname needs to be uppercase.\r\n",
							"Console.WriteLine($\"Help:\\n{TokenLibrary.help()}\");"
						],
						"outputs": [],
						"execution_count": 38
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Miscellaneous Helpers\n",
							"Learn about some internal functions offered by using .NET for Spark."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"// Curious about the version of Spark .NET currently installed?\r\n",
							"// Let's use the following method to find out!\r\n",
							"using Microsoft.Spark.Experimental.Sql;\r\n",
							"spark.GetAssemblyInfo()"
						],
						"outputs": [],
						"execution_count": 39
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"// Current version of the dotnet-interactive REPL.\r\n",
							"#!about"
						],
						"outputs": [],
						"execution_count": 40
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"// We can even run powershell core commands\r\n",
							"#!pwsh\r\n",
							"cat /etc/hosts"
						],
						"outputs": [],
						"execution_count": 41
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"// We can also run F# code\r\n",
							"#!fsharp\r\n",
							"open System\r\n",
							"printfn \"Hello World from F#!\""
						],
						"outputs": [],
						"execution_count": 42
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"// Whatever code is deemed invalid by the C# Compiler, is invalid here too \n",
							"var z = 12345"
						],
						"outputs": [],
						"execution_count": 43
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"// You could write code that throws exceptions and they bubble up to the notebook\n",
							"throw new Exception(\"watzzz\");"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "rksparkPool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/aa62e2f1-b843-4840-bcdf-64081ba7e680/resourceGroups/Day4RG/providers/Microsoft.Synapse/workspaces/rksynapseworkspace03/bigDataPools/rksparkPool",
						"name": "rksparkPool",
						"type": "Spark",
						"endpoint": "https://rksynapseworkspace03.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/rksparkPool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\r\n",
							"df = spark.read.load('abfss://rkadlssynapse@rkadlssynapse.blob.core.windows.net/NYCTripSmall.parquet', format='parquet')\r\n",
							"display(df.limit(10))"
						],
						"outputs": [],
						"execution_count": 1
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/default')]",
			"type": "Microsoft.Synapse/workspaces/managedVirtualNetworks",
			"apiVersion": "2019-06-01-preview",
			"properties": {},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/default/synapse-ws-custstgacct--rksynapseworkspace03-rkadlssynapse')]",
			"type": "Microsoft.Synapse/workspaces/managedVirtualNetworks/managedPrivateEndpoints",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"privateLinkResourceId": "/subscriptions/aa62e2f1-b843-4840-bcdf-64081ba7e680/resourceGroups/Day4RG/providers/Microsoft.Storage/storageAccounts/rkadlssynapse",
				"groupId": "dfs",
				"fqdns": [
					"rkadlssynapse.dfs.core.windows.net"
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/managedVirtualNetworks/default')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/default/synapse-ws-sql--rksynapseanal')]",
			"type": "Microsoft.Synapse/workspaces/managedVirtualNetworks/managedPrivateEndpoints",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"privateLinkResourceId": "/subscriptions/aa62e2f1-b843-4840-bcdf-64081ba7e680/resourceGroups/Day3RG/providers/Microsoft.Synapse/workspaces/rksynapseanal",
				"groupId": "sql",
				"fqdns": [
					"rksynapseanal.8419584c-7df0-4763-9e7b-a28bcfd6073e.sql.azuresynapse.net"
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/managedVirtualNetworks/default')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/default/synapse-ws-sql--rksynapseworkspace03')]",
			"type": "Microsoft.Synapse/workspaces/managedVirtualNetworks/managedPrivateEndpoints",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"privateLinkResourceId": "/subscriptions/aa62e2f1-b843-4840-bcdf-64081ba7e680/resourceGroups/Day4RG/providers/Microsoft.Synapse/workspaces/rksynapseworkspace03",
				"groupId": "sql",
				"fqdns": [
					"rksynapseworkspace03.d95e89f3-a421-44fa-a9f6-679f8a0fe7f0.sql.azuresynapse.net"
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/managedVirtualNetworks/default')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/default/synapse-ws-sqlOnDemand--rksynapseanal')]",
			"type": "Microsoft.Synapse/workspaces/managedVirtualNetworks/managedPrivateEndpoints",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"privateLinkResourceId": "/subscriptions/aa62e2f1-b843-4840-bcdf-64081ba7e680/resourceGroups/Day3RG/providers/Microsoft.Synapse/workspaces/rksynapseanal",
				"groupId": "sqlOnDemand",
				"fqdns": [
					"rksynapseanal-ondemand.8419584c-7df0-4763-9e7b-a28bcfd6073e.sql.azuresynapse.net"
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/managedVirtualNetworks/default')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/default/synapse-ws-sqlOnDemand--rksynapseworkspace03')]",
			"type": "Microsoft.Synapse/workspaces/managedVirtualNetworks/managedPrivateEndpoints",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"privateLinkResourceId": "/subscriptions/aa62e2f1-b843-4840-bcdf-64081ba7e680/resourceGroups/Day4RG/providers/Microsoft.Synapse/workspaces/rksynapseworkspace03",
				"groupId": "sqlOnDemand",
				"fqdns": [
					"rksynapseworkspace03-ondemand.d95e89f3-a421-44fa-a9f6-679f8a0fe7f0.sql.azuresynapse.net"
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/managedVirtualNetworks/default')]"
			]
		}
	]
}